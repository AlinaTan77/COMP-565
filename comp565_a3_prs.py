# -*- coding: utf-8 -*-
"""COMP565 A3 prs.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vZqQe8fcgiWuH0a7DWAnIHkLNYDOg_bs
"""

from google.colab import drive
drive.mount('/content/gdrive')

import pandas as pd
import numpy as np
import math
import itertools
from itertools import combinations
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

"""Implement the expectation-maximization algorithm

# E step
"""

def e_step(LD_cp, beta_marginal, tau_e, tau_beta, pi, M, N_train, snp_list):

     
    for j, snp_j in enumerate(snp_list):
        # Calculate tau_j
        tau_j = (N_train * tau_e) + tau_beta
        
        # Get the indices of the SNP j row and other rows
        idx_j = LD_cp["SNP_name"] == snp_j
        idx_other = LD_cp["SNP_name"] != snp_j

        # Calculate sum_i
        gamma_vec = LD_cp.loc[idx_other, ["gamma"]].values # 99*1
        mu_vec = LD_cp.loc[idx_other, ["mu"]].values # 99*1
        rij_vec = LD_cp.loc[idx_other, [snp_j]].values # 99*1
        gamma_mu_vec = np.multiply(gamma_vec,mu_vec)
        gamma_mu_rij_vec = np.multiply(gamma_mu_vec,rij_vec)
        sum_i = (gamma_mu_rij_vec).sum() 

        # Calculate mu_j
        beta_marginal_j = beta_marginal.loc[beta_marginal["SNP_name"] == snp_j, "V1"].values[0]
        mu_j = N_train * (tau_e / tau_j) * (beta_marginal_j - sum_i)
        
        # Calculate gamma_j
        u_j = math.log(pi / (1-pi)) + (1/2) * (math.log(tau_beta / tau_j)) + (tau_j / 2) * (mu_j ** 2)
        gamma_j = 1 / (1 + np.exp(-u_j))
        
        # Update LD_cp with the new values
        LD_cp.at[idx_j, "tau"] = tau_j

        calculation_matrix = LD_cp[["SNP_name", snp_j, "gamma", "mu", "tau"]]

        LD_cp.at[idx_j,"mu"] = mu_j
        LD_cp.at[idx_j,"gamma"] = gamma_j
        
    posteriors_matrix = calculation_matrix.copy()
    posteriors_matrix["gamma"] = np.clip(posteriors_matrix["gamma"], 0.01, 0.99)
    
    return posteriors_matrix

"""# M step"""

def m_step(posteriors_matrix, beta_marginal, LD):
    # Update tau_beta
    sum_1 = posteriors_matrix["gamma"] * (posteriors_matrix["mu"]**2 + posteriors_matrix["tau"]**(-1))
    tau_beta = (sum_1.sum() / posteriors_matrix["gamma"].sum())**(-1)

    # Update pi
    pi = posteriors_matrix["gamma"].sum() / len(posteriors_matrix)

    # Get beta marginal as np_array
    beta_marginal_np = beta_marginal["V1"].values

    return beta_marginal_np, tau_beta, pi, posteriors_matrix

"""#ELBO calculation"""

def cal_elbo(N_train, tau_e, posteriors_matrix, beta_marginal_np, tau_beta, LD_cp, LD, snp_list):
    ## Calculate 1st ELBO term
    elbo1_1 = 0 # as tau_e = 1
    
    elbo1_2 = -(tau_e/2) * N_train

    gm_mu = posteriors_matrix["gamma"].multiply(posteriors_matrix["mu"])

    elbo1_3 = tau_e * N_train * np.dot(gm_mu, beta_marginal_np)
    
    elbo1_4 = (-tau_e/2) * N_train * (posteriors_matrix["gamma"] * (posteriors_matrix["mu"]**2 + (1/posteriors_matrix["tau"]))).sum()

    R = LD.iloc[:, 1:].values
    sum_jk = 0
    for j in range(M):
      for k in range(j+1, M):
        gamma_j = posteriors_matrix["gamma"][j]
        mu_beta_j = posteriors_matrix["mu"][j]
        gamma_k = posteriors_matrix["gamma"][k]
        mu_beta_k = posteriors_matrix["mu"][k]
        r_jk = R[j, k]
        xtx = r_jk*N_train
        sum_jk = sum_jk + (gamma_j * mu_beta_j * gamma_k * mu_beta_k * xtx)
    elbo1_5 = - tau_e * sum_jk

    
    elbo1 = elbo1_2 + elbo1_3 + elbo1_4 + elbo1_5


    ## Calculate 2nd ELBO term 
    elbo2 = -(tau_beta/2)*posteriors_matrix["gamma"]*(posteriors_matrix["mu"]**2 + posteriors_matrix["tau"]**(-1))
    elbo2 = elbo2.sum()

    ## Calculate 3rd ELBO term
    elbo3 = ((1-posteriors_matrix["gamma"])*np.log(1-pi) + posteriors_matrix["gamma"]*np.log(pi)).sum()

    ## Calculate 4th ELBO term 
    elbo4 = (-1/2)*(posteriors_matrix["gamma"]*np.log(tau_beta)).sum()

    ## Calculate 5th ELBO term
    elbo5 = (posteriors_matrix["gamma"]*np.log(posteriors_matrix["gamma"]) + (1-posteriors_matrix["gamma"])*np.log(1-posteriors_matrix["gamma"])).sum()


    elbo = elbo1 + elbo2 + elbo3 - elbo4 - elbo5

    return elbo

"""# Run experiment of EM and ELBO"""

LD = pd.read_csv("/content/gdrive/My Drive/COMP 565/A3/data/LD.csv")
beta_marginal = pd.read_csv("/content/gdrive/My Drive/COMP 565/A3/data/beta_marginal.csv")

# Rename the first columns
beta_marginal.rename(columns={'Unnamed: 0':'SNP_name'}, inplace=True)
LD.rename(columns={'Unnamed: 0':'SNP_name'}, inplace=True)

# initialize values
M = 100  # number of SNPs
N_train = 439
N_test = 50

# Create a list of the SNPs 
snp_list = beta_marginal["SNP_name"].to_list()


# initialize values for the hyperparameters
tau_e = 1
tau_beta = 200
pi = 0.01

# Create a copy of the input dataframe to avoid modifying it directly
LD_cp = LD.copy()

# initialize values for the posterior estimates for all SNPs
LD_cp["mu"] = 0.0
LD_cp["tau"] = 1.0
LD_cp["gamma"] = 0.01


elbo_list = []
for ite in range(10): # for 10 iterations
     #print("iteration: ", ite+1)

     ## E step
     posteriors_matrix = e_step(LD_cp, beta_marginal, tau_e, tau_beta, pi, M, N_train, snp_list)

     #print (posteriors_matrix)
     
     ## M step
     beta_marginal_np, tau_beta, pi, posteriors_matrix = m_step(posteriors_matrix, beta_marginal, LD)
     
     ## Calculate ELBO term by term
     elbo = cal_elbo(N_train, tau_e, posteriors_matrix, beta_marginal_np, tau_beta, LD_cp, LD, snp_list)
     #print("ELBO: ", elbo)
    
     elbo_list.append(elbo)
print("ELBO: ", elbo_list)

##Generate the plot
# Define x and y values for the plot
x_values = list(range(1, 11))
y_values = elbo_list

# Plot the values as green dots
plt.plot(x_values, y_values, 'ko')

# Add labels to the x and y axes
plt.xlabel('Iteration')
plt.ylabel('ELBO')

# Add a title to the figure
plt.title("Evidence lower bound as a function of EM iteration")
plt.grid()
# Display the plot
plt.show()

"""# Evaluating PRS prediction"""

X_train= pd.read_csv("/content/gdrive/My Drive/COMP 565/A3/data/X_train.csv")
y_train =pd.read_csv("/content/gdrive/My Drive/COMP 565/A3/data/y_train.csv")

X_test= pd.read_csv("/content/gdrive/My Drive/COMP 565/A3/data/X_test.csv")
y_test =pd.read_csv("/content/gdrive/My Drive/COMP 565/A3/data/y_test.csv")

#elementwised product of gamma and mu
gm_mu = posteriors_matrix["gamma"].multiply(posteriors_matrix["mu"])

# Calculate y_hat_train
X_train = X_train.iloc[:, 1:].values
y_train = y_train.iloc[:, 1].values
y_hat_train = np.dot(X_train, gm_mu).astype('float64')

# Calculate y_hat_test
X_test = X_test.iloc[:, 1:].values
y_test = y_test.iloc[:, 1].values
y_hat_test = np.dot(X_test, gm_mu).astype('float64')


# plot and calculate the Pearson Correlation Coefficient
fig, axs = plt.subplots(1, 2, figsize=(10, 4))

model1 = np.polyfit(y_hat_train, y_train, 1)
predict1 = np.poly1d(model1)
linear = predict1(y_hat_train)
r_train = np.sqrt(r2_score(y_train, linear))
axs[0].scatter(y_hat_train, y_train, c='k', alpha=1, s=4)
axs[0].plot(y_hat_train, linear, c='b')
axs[0].set_ylim([-3, 3.2])
axs[0].set_title("Train: PCC = {:.2f}".format(r_train))
axs[0].set_xlabel("Predicted phenotype")
axs[0].set_ylabel("True phenotype")

model2 = np.polyfit(y_hat_test, y_test, 1)
predict2 = np.poly1d(model2)
linear2 = predict2(y_hat_test)
r_test = np.sqrt(r2_score(y_test, linear2))
axs[1].scatter(y_hat_test, y_test, c='k', alpha=1, s=4)
axs[1].plot(y_hat_test, linear2, c='b')
axs[1].set_ylim([-3, 3.2])
axs[1].set_title("Test: PCC = {:.2f}".format(r_test))
axs[1].set_xlabel("Predicted phenotype")

plt.tight_layout()
plt.show()

"""# Evaluating fine-mapping"""

#plot the inferred PIP with causal SNPs colored in red
color = np.where(posteriors_matrix["gamma"]>0.05,'r','b')
posteriors_matrix.plot.scatter(x="SNP_name", y="gamma", alpha=1, c=color)
plt.title("Inferred PIP. Causal SNPs coloured in red")
plt.show()